#Healthchekcs won't work because the images used don't contain wget or curl
services:
    postgres:
        container_name: postgres_container
        image: postgres:14
        mem_limit: 700m
        environment:
            POSTGRES_USER: malcolm
            POSTGRES_PASSWORD: malcolm
            PGDATA: /data/postgres
        volumes:
            - postgres:/data/postgres
            - ./init-multi-db.sql:/docker-entrypoint-initdb.d/init.sql # To create multiple DBs at startup
        ports:
            - "5432:5432"
        networks:
            - backend
        restart: unless-stopped
    pgadmin:
        container_name: pgadmin_container
        image: dpage/pgadmin4
        mem_limit: 700m
        environment:
            PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-malcolm@mail.com}
            PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD:-malcolm}
            PGADMIN_CONFIG_SERVER_MODE: "False"
        volumes:
            - pgadmin:/var/lib/pgadmin
        ports:
            - "5050:80"
        networks:
            - backend
        restart: unless-stopped

    keycloak:
        container_name: keycloak
        image: quay.io/keycloak/keycloak:26.5.1
        mem_limit: 700m
        volumes:
            - keycloak:/opt/keycloak/data
        ports:
            - "8443:8080"
        environment:
            KC_BOOTSTRAP_ADMIN_USERNAME: admin
            KC_BOOTSTRAP_ADMIN_PASSWORD: admin
        #      PROXY_ADDRESS_FORWARDING: "true"
        #      KEYCLOAK_FRONTEND_URL: "http://localhost:8443"
        command: ["start-dev"]
        networks:
            - backend

    mongo:
        image: mongodb/mongodb-community-server:latest
        container_name: mongo
        mem_limit: 700m
        ports:
            - "27017:27017"
        networks:
            - backend

    zookeeper:
        image: confluentinc/cp-zookeeper:7.5.0
        container_name: zookeeper
        ports:
            - "2181:2181"
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000
        networks:
            - backend

    kafka:
        image: confluentinc/cp-kafka:7.5.0
        container_name: kafka
        ports:
            - "9092:9092"
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        depends_on:
            - zookeeper
        networks:
            - backend

    rabbitmq:
        container_name: rabbitmq
        image: rabbitmq:4-management
        environment:
            RABBITMQ_DEFAULT_USER: guest
            RABBITMQ_DEFAULT_PASS: guest
        ports:
            - "5672:5672"
            - "15672:15672"
        networks:
            - backend
        restart: unless-stopped

    config-server:
        #        build: ../config-server
        image: store/config-server
        #        image: malcolmgx/config-server # If you want to use the image built with jib
        container_name: config-server
        ports:
            - 8888:8888
        environment: # Docker will pick the environment variables from the .env file
            - SPRING_CLOUD_CONFIG_SERVER_NATIVE_SEARCH-LOCATIONS=/config # (Incase the volume mapping doesn't work)
            - RABBITMQ_HOST=${RABBITMQ_HOST}
            - RABBITMQ_PORT=${RABBITMQ_PORT}
            - RABBITMQ_USERNAME=${RABBITMQ_USERNAME}
            - RABBITMQ_PASSWORD=${RABBITMQ_PASSWORD}
            - RABBITMQ_VHOST=${RABBITMQ_VHOST}
        volumes:
            - ./../config-server/src/main/resources/config:/config
        networks:
            - backend
        healthcheck:
            test:
                [
                    "CMD",
                    "wget",
                    "--spider",
                    "http://localhost:8888/actuator/health",
                ]
            interval: 30s
            timeout: 10s
            retries: 5

    eureka:
        #        build: ../eureka
        image: store/eureka
        #        image: malcolmgx/eureka # If you want to use the image built with jib
        container_name: eureka
        ports:
            - 8761:8761
        networks:
            - backend
        healthcheck:
            test:
                [
                    "CMD",
                    "wget",
                    "--spider",
                    "http://localhost:8761/actuator/health",
                ]
            interval: 30s
            timeout: 10s
            retries: 5

    gateway:
        #        build: ../gateway
        image: store/gateway
        #        image: malcolmgx/gateway # If you want to use the image built with jib
        container_name: gateway
        ports:
            - 8080:8080
        environment:
            - SPRING_PROFILES_ACTIVE=docker
        env_file:
            - ../gateway/docker.env
        networks:
            - backend
        healthcheck:
            test:
                [
                    "CMD",
                    "wget",
                    "--spider",
                    "http://localhost:8080/actuator/health",
                ]
            interval: 30s
            timeout: 10s
            retries: 5
        depends_on:
            config-server:
                condition: service_started
            eureka:
                condition: service_started
            keycloak:
                condition: service_started

    user:
        #        build: ../user
        image: store/user
        #        image: malcolmgx/user # If you want to use the image built with jib
        container_name: user
        ports:
            - 8082:8082
        environment:
            - SPRING_PROFILES_ACTIVE=docker
        env_file:
            - ../user/docker.env
        networks:
            - backend
        healthcheck:
            test:
                [
                    "CMD",
                    "wget",
                    "--spider",
                    "http://localhost:8082/actuator/health",
                ]
            interval: 30s
            timeout: 10s
            retries: 5
        depends_on:
            config-server:
                condition: service_started
            eureka:
                condition: service_started
            keycloak:
                condition: service_started
        restart: on-failure

    product:
        #        build: ../product
        image: store/product
        #        image: malcolmgx/product # If you want to use the image built with jib
        container_name: product
        ports:
            - 8081:8081
        environment:
            - SPRING_PROFILES_ACTIVE=docker
        env_file:
            - ../product/docker.env
        networks:
            - backend
        healthcheck:
            test:
                [
                    "CMD",
                    "wget",
                    "--spider",
                    "http://localhost:8081/actuator/health",
                ]
            interval: 30s
            timeout: 10s
            retries: 5
        depends_on:
            config-server:
                condition: service_started
            eureka:
                condition: service_started
            keycloak:
                condition: service_started
        restart: on-failure

    order:
        #        build: ../order
        image: store/order
        #        image: malcolmgx/order # If you want to use the image built with jib
        container_name: order
        ports:
            - 8083:8083
        environment:
            - SPRING_PROFILES_ACTIVE=docker
        env_file:
            - ../order/docker.env
        networks:
            - backend
        healthcheck:
            test:
                [
                    "CMD",
                    "wget",
                    "--spider",
                    "http://localhost:8083/actuator/health",
                ]
            interval: 30s
            timeout: 10s
            retries: 5
        depends_on:
            config-server:
                condition: service_started
            eureka:
                condition: service_started
            keycloak:
                condition: service_started
        restart: on-failure

    notification:
        #        build: ../notification
        image: store/notification
        #        image: malcolmgx/notification # If you want to use the image built with jib
        container_name: notification
        ports:
            - 8084:8084
        environment:
            - SPRING_PROFILES_ACTIVE=docker
        env_file:
            - ../notification/docker.env
        networks:
            - backend
        healthcheck:
            test:
                [
                    "CMD",
                    "wget",
                    "--spider",
                    "http://localhost:8084/actuator/health",
                ]
            interval: 30s
            timeout: 10s
            retries: 5
        depends_on:
            config-server:
                condition: service_started
            eureka:
                condition: service_started
            keycloak:
                condition: service_started
            kafka:
                condition: service_started
        restart: on-failure

    read:
        image: grafana/loki:latest
        command: "-config.file=/etc/loki/config.yaml -target=read"
        ports:
            - 3101:3100
            - 7946
            - 9095
        volumes:
            - ./logging/loki-config.yaml:/etc/loki/config.yaml
        depends_on:
            - minio
        healthcheck:
            test:
                [
                    "CMD-SHELL",
                    "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1",
                ]
            interval: 10s
            timeout: 5s
            retries: 5
        networks: &loki-dns
            loki:
                aliases:
                    - loki

    write:
        image: grafana/loki:latest
        command: "-config.file=/etc/loki/config.yaml -target=write"
        ports:
            - 3102:3100
            - 7946
            - 9095
        volumes:
            - ./logging/loki-config.yaml:/etc/loki/config.yaml
        healthcheck:
            test:
                [
                    "CMD-SHELL",
                    "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1",
                ]
            interval: 10s
            timeout: 5s
            retries: 5
        depends_on:
            - minio
        networks:
            <<: *loki-dns

    alloy:
        image: grafana/alloy:latest
        volumes:
            - ./logging/alloy-local-config.yaml:/etc/alloy/config.alloy:ro
            - /var/run/docker.sock:/var/run/docker.sock
            - ../../logs:/logs-parent:ro
        command: run --server.http.listen-addr=0.0.0.0:12345 --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy
        ports:
            - 12345:12345
        depends_on:
            - loki-gateway
        networks:
            - loki

    minio:
        image: minio/minio
        entrypoint:
            - sh
            - -euc
            - |
                mkdir -p /data/loki-data && \
                mkdir -p /data/loki-ruler && \
                minio server /data
        environment:
            - MINIO_ROOT_USER=loki
            - MINIO_ROOT_PASSWORD=supersecret
            - MINIO_PROMETHEUS_AUTH_TYPE=public
            - MINIO_UPDATE=off
        ports:
            - 9000
        volumes:
            - ./.data/minio:/data
        healthcheck:
            test:
                ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 15s
            timeout: 20s
            retries: 5
        networks:
            - loki

    grafana:
        image: grafana/grafana:latest
        environment:
            - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
            - GF_AUTH_ANONYMOUS_ENABLED=true
            - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
        depends_on:
            - loki-gateway
        entrypoint:
            - sh
            - -euc
            - |
                mkdir -p /etc/grafana/provisioning/datasources
                cat <<EOF > /etc/grafana/provisioning/datasources/ds.yaml
                apiVersion: 1
                datasources:
                  - name: Loki
                    type: loki
                    access: proxy
                    url: http://loki-gateway:3100
                    jsonData:
                      httpHeaderName1: "X-Scope-OrgID"
                    secureJsonData:
                      httpHeaderValue1: "tenant1"
                EOF
                /run.sh
        ports:
            - "3000:3000"
        healthcheck:
            test:
                [
                    "CMD-SHELL",
                    "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1",
                ]
            interval: 10s
            timeout: 5s
            retries: 5
        volumes:
            - ./grafana/datasources:/etc/grafana/provisioning/datasources
        networks:
            - loki

    backend:
        image: grafana/loki:latest
        volumes:
            - ./logging/loki-config.yaml:/etc/loki/config.yaml
        ports:
            - "3100"
            - "7946"
        command: "-config.file=/etc/loki/config.yaml -target=backend -legacy-read-mode=false"
        depends_on:
            - loki-gateway
        networks:
            - loki

    prometheus:
        image: prom/prometheus:v2.44.0
        container_name: prometheus
        ports:
            - "9090:9090"
        volumes:
            - ./prometheus/prometheus.yaml:/etc/prometheus/prometheus.yml
        networks:
            - loki

    loki-gateway:
        image: nginx:latest
        depends_on:
            - read
            - write
        entrypoint:
            - sh
            - -euc
            - |
                cat <<EOF > /etc/nginx/nginx.conf
                user  nginx;
                worker_processes  5;  ## Default: 1

                events {
                  worker_connections   1000;
                }

                http {
                  resolver 127.0.0.11;

                  server {
                    listen             3100;

                    location = / {
                      return 200 'OK';
                      auth_basic off;
                    }

                    location = /api/prom/push {
                      proxy_pass       http://write:3100\$$request_uri;
                    }

                    location = /api/prom/tail {
                      proxy_pass       http://read:3100\$$request_uri;
                      proxy_set_header Upgrade \$$http_upgrade;
                      proxy_set_header Connection "upgrade";
                    }

                    location ~ /api/prom/.* {
                      proxy_pass       http://read:3100\$$request_uri;
                    }

                    location = /loki/api/v1/push {
                      proxy_pass       http://write:3100\$$request_uri;
                    }

                    location = /loki/api/v1/tail {
                      proxy_pass       http://read:3100\$$request_uri;
                      proxy_set_header Upgrade \$$http_upgrade;
                      proxy_set_header Connection "upgrade";
                    }

                    location ~ /loki/api/.* {
                      proxy_pass       http://read:3100\$$request_uri;
                    }
                  }
                }
                EOF
                /docker-entrypoint.sh nginx -g "daemon off;"
        ports:
            - "3100:3100"
        healthcheck:
            test: ["CMD", "service", "nginx", "status"]
            interval: 10s
            timeout: 5s
            retries: 5
        networks:
            - loki

    zipkin:
        container_name: zipkin
        image: openzipkin/zipkin
        ports:
            - 9411:9411
        networks:
            - backend
            - loki

    flog:
        image: mingrammer/flog
        command: -f json -d 200ms -l
        networks:
            - loki

    redis:
        image: redis:latest
        restart: unless-stopped
        ports:
            - "6379:6379"

networks:
    backend:
        driver: bridge
    loki:
        driver: bridge

volumes:
    postgres:
    pgadmin:
    keycloak:
